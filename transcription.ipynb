{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI imports\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import whisper\n",
    "import noisereduce as nr \n",
    "\n",
    "# data processing imports\n",
    "import pydub\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import pathlib\n",
    "import shutil \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API keys\n",
    "with open('api_keys.json') as json_file:\n",
    "    api_keys = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Audio File and Convert to WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_name_ext(filename):\n",
    "    '''\n",
    "    Given a full filename, returns the path, stem (name), and extension.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    filename : string\n",
    "        The full filename of the file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    path : string\n",
    "        Parent directory of the file\n",
    "    \n",
    "    name : string\n",
    "        The name of the file, excluding the extension. Does not include the parent directory.\n",
    "\n",
    "    extension : string\n",
    "        The file extension.\n",
    "    '''\n",
    "    path = os.path.dirname(filename)\n",
    "    name = pathlib.Path(filename).stem\n",
    "    extension = pathlib.Path(filename).suffix.replace('.', '')\n",
    "\n",
    "    return path, name, extension\n",
    "\n",
    "def import_audio(filename):\n",
    "    ''' \n",
    "    Given a filename of an audio file, creates an output folder and converts a copy to .wav format.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    filename : string\n",
    "        The full filename of the file.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output_filename : string\n",
    "        Full path to the copy in the output folder.\n",
    "\n",
    "    '''\n",
    "    # get file name and extension\n",
    "    path, name, extension = get_path_name_ext(filename)\n",
    "\n",
    "    # create an output path for the file if necessary\n",
    "    output_path = os.path.join('output', name)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # final filename\n",
    "    output_filename = os.path.join(output_path, f'{name}.wav')\n",
    "\n",
    "    # convert to .wav file if necessary and save in the output folder\n",
    "    sound = pydub.AudioSegment.from_file(filename, format=extension)\n",
    "    sound.export(output_filename, format='wav')\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "original_filename = os.path.join('input', 'Conversation_Grace_Alex_2.m4a')\n",
    "output_filename = import_audio(original_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoise Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_audio(filename):\n",
    "    ''' \n",
    "    Given a .wav filename, removes background noise from the audio and saves a denoised copy.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    filename : string\n",
    "        The full filename of the file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    output_filename : string\n",
    "        Full path to the denoised copy in the output folder.\n",
    "    '''\n",
    "    # get file name and extension\n",
    "    path, name, extension = get_path_name_ext(filename)\n",
    "\n",
    "    # import the file and reduce the noise\n",
    "    rate, data = wavfile.read(filename)\n",
    "    reduced_noise = nr.reduce_noise(y=data, sr=rate, stationary=False, prop_decrease=0.65)\n",
    "\n",
    "    # output the data\n",
    "    output_name = f'{name}_DENOISED.wav'\n",
    "    output_filename = os.path.join(path, output_name)\n",
    "    wavfile.write(output_filename, rate, reduced_noise)\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "output_filename = denoise_audio(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Speaker Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to import diarization model and perform speaker diarization\n",
    "def diarize_audio(filename, api_key, n_speakers=None):\n",
    "    ''' \n",
    "    Given a .wav filename, performs speaker diarization.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    filename : string\n",
    "        The full filename of the file.\n",
    "\n",
    "    api_key : string\n",
    "        Your HuggingFace API key.\n",
    "\n",
    "    n_speakers : int\n",
    "        Number of speakers in the audio clip. If None, the algorithm will attempt to guess the number of speakers.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    turn_list : list[list[string, float, float]]\n",
    "        A list of turns taken by speakers in the audio. Each turn is a list that contains the speaker label, beginning in seconds, \n",
    "        and end in seconds.\n",
    "    '''\n",
    "    # import the speaker diarization pipeline\n",
    "    diarization_model = Pipeline.from_pretrained(\n",
    "        'pyannote/speaker-diarization-3.1',\n",
    "        use_auth_token=api_key\n",
    "    )\n",
    "    diarization_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "    # implement speaker diarization model\n",
    "    diarization_results = diarization_model(filename, num_speakers=n_speakers)\n",
    "\n",
    "    # create turn list\n",
    "    turn_list = []\n",
    "    for turn, _, speaker in diarization_results.itertracks(yield_label=True):\n",
    "        turn_list.append([speaker, turn.start, turn.end])\n",
    "\n",
    "    return turn_list\n",
    "\n",
    "turn_list = diarize_audio(output_filename, api_keys['pyannote'], n_speakers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Process the Speaker Turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to post-process diarization results (remove short clips, limit overlaps, join consecutive turns)\n",
    "def consolidate_turns(turn_list, removal_threshold=0.10, max_overlap=1.0):\n",
    "    ''' \n",
    "    Given a turn list, filters out short turns, joins consecutive turns, and minimizes turn overlap between speakers.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    turn_list : list[list[string, float, float]]\n",
    "        A list of turns taken by speakers in the audio. Each turn is a list that contains the speaker label, beginning in seconds, \n",
    "        and end in seconds.\n",
    "\n",
    "    removal_threshold : float\n",
    "        Minimum threshold for turn length in seconds. If a turn is less than this length, it is removed.\n",
    "\n",
    "    max_overlap : float\n",
    "        Maximum overlap of turns in seconds. If two turns overlap more than this, the beginning of the latter turn is truncated\n",
    "        to the maximum overlap value specifed.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    consolidated_turn_list : list[list]\n",
    "        A list of turns taken by speakers in the audio, post-consolidation. Each turn is a list that contains the speaker label, \n",
    "        beginning in seconds, and end in seconds.\n",
    "    '''\n",
    "    # iterate through the filtering, consolidating, and overlap removal steps until convergence\n",
    "    prev_turn_list_length = -1\n",
    "    while len(turn_list) != prev_turn_list_length:\n",
    "        # reset the first pass flag and the previous turn list length\n",
    "        prev_turn_list_length = len(turn_list)\n",
    "\n",
    "        # remove any turns that are less than the threshold\n",
    "        filtered_turn_list = []\n",
    "        for t in turn_list:\n",
    "            if (t[2] - t[1]) >= removal_threshold:\n",
    "                filtered_turn_list.append(t)\n",
    "\n",
    "        # join consecutive turns\n",
    "        consolidated_turn_list = []\n",
    "        current_turn = None\n",
    "        for t in filtered_turn_list:\n",
    "            if current_turn is None:\n",
    "                current_turn = [t[0], t[1], t[2]]\n",
    "            elif t[0] == current_turn[0]:\n",
    "                current_turn[2] = t[2]\n",
    "            else:\n",
    "                consolidated_turn_list.append(current_turn)\n",
    "                current_turn = [t[0], t[1], t[2]]\n",
    "\n",
    "        consolidated_turn_list.append(current_turn)\n",
    "\n",
    "        # limiting overlap threshold\n",
    "        prev_end_time = 0\n",
    "        for t in consolidated_turn_list:\n",
    "            t[1] = max(prev_end_time - max_overlap, t[1])\n",
    "            prev_end_time = t[2]\n",
    "\n",
    "        # resetting the turn list with the consolidated turn list\n",
    "        turn_list = consolidated_turn_list\n",
    "\n",
    "    return consolidated_turn_list\n",
    "\n",
    "consolidated_turns = consolidate_turns(turn_list)\n",
    "print(consolidated_turns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip Turns from the Main Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_audio_segments(filename, turn_list):\n",
    "    ''' \n",
    "    Given a filename of the full conversation and a turn list, creates a series of audio clips in .wav format within an output\n",
    "    sub-directory where each audio clip contains a speaking turn.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    filename : string\n",
    "        Full filename of the .wav audio file to be clipped.\n",
    "\n",
    "    turn_list : list[list[string, float, float]]\n",
    "        A list of turns taken by speakers in the audio. Each turn is a list that contains the speaker label, beginning in seconds, \n",
    "        and end in seconds.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    segment_output_path : string\n",
    "        The directory of the sub-directory containing all of the clipped audio segments.\n",
    "    '''\n",
    "    # get file name and extension\n",
    "    path, name, extension = get_path_name_ext(filename)\n",
    "\n",
    "    # create output directory for all segments, and remove existing if necessary\n",
    "    segment_output_path = os.path.join(path, 'diarization_segments')\n",
    "    if os.path.exists(segment_output_path):\n",
    "        shutil.rmtree(segment_output_path)\n",
    "        \n",
    "    os.makedirs(segment_output_path)\n",
    "    \n",
    "    # Open the .wav file\n",
    "    audio = pydub.AudioSegment.from_file(filename)\n",
    "\n",
    "    # Define start and end times in seconds\n",
    "    segment_filenames = []\n",
    "    for i, t in enumerate(turn_list):\n",
    "        start_time = t[1]\n",
    "        end_time = t[2]\n",
    "\n",
    "        # Extract the segment\n",
    "        segment = audio[start_time * 1000: end_time * 1000]\n",
    "\n",
    "        # Write the segment to a new .wav file\n",
    "        current_segment_filename = os.path.join(segment_output_path, f'segment_{str(i).zfill(6)}.wav')\n",
    "        segment_filenames.append(current_segment_filename)\n",
    "        segment.export(current_segment_filename, format='wav')\n",
    "\n",
    "    return segment_output_path\n",
    "\n",
    "segment_output_path = clip_audio_segments(output_filename, consolidated_turns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Transcript of Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to import transcription model and transcribe the diarized segments\n",
    "def transcribe_segments(segment_output_path, turn_list):\n",
    "    ''' \n",
    "    Given the directory containing the audio clips of individual turns and a corresponding turn list, the individual\n",
    "    audio clips are transcribed with the Whisper model and a transcript of the full conversation is saved to the output folder.\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    segment_output_path : string\n",
    "        Directory that contains the individual audio clips in .wav format.\n",
    "\n",
    "    turn_list : list[list[string, float, float]]\n",
    "        A list of turns taken by speakers in the audio. Each turn is a list that contains the speaker label, beginning in seconds, \n",
    "        and end in seconds.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    transcript : string\n",
    "        The full transcript of the conversation.\n",
    "    '''\n",
    "    # import whisper model\n",
    "    whisper_model = whisper.load_model('base', device='cuda')\n",
    "\n",
    "    # get parent directory\n",
    "    output_path = os.path.dirname(segment_output_path)\n",
    "\n",
    "    # get list of segment filenames\n",
    "    segment_list = os.listdir(segment_output_path)\n",
    "\n",
    "    # transcribe each segment\n",
    "    segment_text_list = []\n",
    "    for segment in segment_list:\n",
    "        segment_filename = os.path.join(segment_output_path, segment)\n",
    "        result = whisper_model.transcribe(segment_filename)\n",
    "        segment_text_list.append(result[\"text\"])\n",
    "        print(segment)\n",
    "\n",
    "    # create transcript with speaker labels\n",
    "    transcript = ''\n",
    "    for turn, segment_text in zip(turn_list, segment_text_list):\n",
    "        transcript += f'{turn[0]} : {segment_text}\\n'\n",
    "\n",
    "    # write transcript to file\n",
    "    transcript_filename = os.path.join(output_path, 'transcript.txt')\n",
    "    with open(transcript_filename, 'w') as text_filename:\n",
    "        print(transcript, file=text_filename)\n",
    "\n",
    "    return transcript\n",
    "\n",
    "transcript = transcribe_segments(segment_output_path, consolidated_turns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lazarus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
